{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb9db43-1898-4802-a65d-bde103aefe25",
   "metadata": {},
   "source": [
    "#### Title: Machine Learning Classifier for News Articles\n",
    "#### Author: Brian Castillo\n",
    "#### Date: 14 Sep 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfadc8e1-0dd2-4090-ac4d-6577a5d09bd6",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35818349-c24a-49a1-8bbd-e5de8c7b0f16",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "- Data Collection: Gathering news articles from various categories.\n",
    "- Data Preprocessing: Cleaning and preparing the text data for analysis.\n",
    "- Feature Engineering: Transforming the text into a format that can be fed into ML algorithms.\n",
    "- Model Building: Creating a machine learning model to classify the articles into different categories.\n",
    "- Model Evaluation: Assessing the model's performance through metrics like accuracy, precision, recall, etc.\n",
    "- Interpretation: Analyzing the results and understanding how the model is making its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d6568f-eb81-402a-af9c-baafc3397591",
   "metadata": {},
   "source": [
    "__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fef2c003-d5fd-4ea1-8d1e-791d3080c022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1cd8d5-4498-453e-83d4-2127c2bb1c41",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "Using a dataset with ~200k articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c4c2d74-d223-478e-b08b-cd9c8b9f54b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffpost.com/entry/covid-boosters-...</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "      <td>Carla K. Johnson, AP</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffpost.com/entry/american-airlin...</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "      <td>Mary Papenfuss</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-tweets...</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "      <td>Elyse Wanshel</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-parent...</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "      <td>Caroline Bologna</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffpost.com/entry/amy-cooper-lose...</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "      <td>Nina Golgowski</td>\n",
       "      <td>2022-09-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  https://www.huffpost.com/entry/covid-boosters-...   \n",
       "1  https://www.huffpost.com/entry/american-airlin...   \n",
       "2  https://www.huffpost.com/entry/funniest-tweets...   \n",
       "3  https://www.huffpost.com/entry/funniest-parent...   \n",
       "4  https://www.huffpost.com/entry/amy-cooper-lose...   \n",
       "\n",
       "                                            headline   category  \\\n",
       "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
       "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
       "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
       "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
       "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
       "\n",
       "                                   short_description               authors  \\\n",
       "0  Health experts said it is too early to predict...  Carla K. Johnson, AP   \n",
       "1  He was subdued by passengers and crew when he ...        Mary Papenfuss   \n",
       "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel   \n",
       "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna   \n",
       "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski   \n",
       "\n",
       "         date  \n",
       "0  2022-09-23  \n",
       "1  2022-09-23  \n",
       "2  2022-09-23  \n",
       "3  2022-09-23  \n",
       "4  2022-09-22  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty list to store the JSON objects\n",
    "data_list = []\n",
    "\n",
    "# Open the file and read line by line\n",
    "with open('News_Category_Dataset_v3.json', 'r') as f:\n",
    "    for line in f:\n",
    "        # Parse each line as a JSON object and append to list\n",
    "        data_list.append(json.loads(line))\n",
    "\n",
    "# Convert the list to a Pandas DataFrame\n",
    "news_df = pd.DataFrame(data_list)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b19e11-b79d-4204-80dc-f664518f452b",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e437bc-bb89-47ac-b616-c11efc5e85c5",
   "metadata": {},
   "source": [
    "### Data Preproccessing\n",
    "- Removing special characters\n",
    "- Converting all text to lower case\n",
    "- Removing the 'categories' column since this will be a maching learning algorithm to categorize articles\n",
    "- Any other necessary text cleaning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee0fcc4a-738e-44cf-b996-abf8f0f8967d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "link                 0\n",
       "headline             0\n",
       "category             0\n",
       "short_description    0\n",
       "authors              0\n",
       "date                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for any missing values\n",
    "missing_values = news_df.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "165fa177-404f-47b6-aa4a-b410b813d779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brian\\AppData\\Local\\Temp\\ipykernel_12504\\401596953.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  news_df['headline_nospecialchar'] = news_df['headline'].str.replace('[^\\w\\s]', '')\n"
     ]
    }
   ],
   "source": [
    "# Converting the headlines to lowercase, removing special characters, and tokenizing each word\n",
    "news_df['headline_nospecialchar'] = news_df['headline'].str.replace('[^\\w\\s]', '')\n",
    "\n",
    "news_df['headline_lower'] = news_df['headline_nospecialchar'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ca022a99-1c99-4363-818c-4f5a95243ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a token for each word\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "news_df['headline_tokens'] = news_df['headline_lower'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "03684d60-0a04-40de-b492-3664fde8b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "news_df['headline_filtered'] = news_df['headline_tokens'].apply(lambda x: [word for word in x if word.lower() not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "abaac34f-43c2-46cc-a494-9f58f7520296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization (converting a word to its root form, ex: programming -> program)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "news_df['headline_lemmatized'] = news_df['headline_filtered'].apply(lambda x: [lemmatizer.lemmatize(word.lower()) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "edd87b8a-03dd-4454-83d2-bde9365c4d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorization \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(news_df['headline_lemmatized'].apply(lambda x: ' '.join(x))) # This is my feature matrix, It is a numerical representation of the text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3562e3-e725-4322-bf9d-f474dde6fa44",
   "metadata": {},
   "source": [
    "________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2531945-a5da-4d69-bd52-dc0762e2a386",
   "metadata": {},
   "source": [
    "### Building the classifier\n",
    "\n",
    "I will be using the **Random Forest** classifier\n",
    "\n",
    "- Split the dataset into training and testing sets.\n",
    "- Train a Random Forest classifier on the training set.\n",
    "- Evaluate the model on the testing set.\n",
    "- Examine feature importance (which words are most indicative of each category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c7cd1889-26e4-40e7-8270-044464692b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (146668, 61302)\n",
      "Testing set size: (62859, 61302)\n"
     ]
    }
   ],
   "source": [
    "# Creating the training sets\n",
    "y = news_df['category'] # To train the model the model must know the correct answer\n",
    "\n",
    "# Splitting the data using sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data into 70% training and 30% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Testing set size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "beefbdfc-146b-42fc-aeb3-101e51a92b3b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.5333046978157463\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "          ARTS       0.34      0.21      0.26       438\n",
      "ARTS & CULTURE       0.36      0.09      0.14       388\n",
      "  BLACK VOICES       0.53      0.25      0.34      1378\n",
      "      BUSINESS       0.46      0.37      0.41      1796\n",
      "       COLLEGE       0.35      0.32      0.34       297\n",
      "        COMEDY       0.57      0.35      0.43      1620\n",
      "         CRIME       0.44      0.45      0.44      1094\n",
      "CULTURE & ARTS       0.68      0.20      0.31       309\n",
      "       DIVORCE       0.76      0.60      0.67      1013\n",
      "     EDUCATION       0.34      0.25      0.29       315\n",
      " ENTERTAINMENT       0.54      0.67      0.60      5148\n",
      "   ENVIRONMENT       0.51      0.12      0.20       442\n",
      "         FIFTY       0.22      0.06      0.09       413\n",
      "  FOOD & DRINK       0.54      0.69      0.60      1896\n",
      "     GOOD NEWS       0.27      0.08      0.12       402\n",
      "         GREEN       0.39      0.27      0.31       781\n",
      "HEALTHY LIVING       0.30      0.12      0.17      2001\n",
      " HOME & LIVING       0.65      0.62      0.64      1292\n",
      "        IMPACT       0.32      0.12      0.18      1038\n",
      " LATINO VOICES       0.77      0.16      0.26       350\n",
      "         MEDIA       0.53      0.30      0.38       880\n",
      "         MONEY       0.53      0.25      0.34       534\n",
      "     PARENTING       0.43      0.60      0.50      2673\n",
      "       PARENTS       0.41      0.12      0.19      1200\n",
      "      POLITICS       0.61      0.85      0.71     10653\n",
      "  QUEER VOICES       0.75      0.59      0.66      1899\n",
      "      RELIGION       0.55      0.37      0.44       785\n",
      "       SCIENCE       0.56      0.35      0.43       629\n",
      "        SPORTS       0.60      0.55      0.57      1532\n",
      "         STYLE       0.38      0.08      0.14       676\n",
      "STYLE & BEAUTY       0.62      0.76      0.69      3010\n",
      "         TASTE       0.37      0.09      0.15       619\n",
      "          TECH       0.49      0.32      0.39       611\n",
      " THE WORLDPOST       0.50      0.30      0.37      1103\n",
      "        TRAVEL       0.59      0.66      0.63      3073\n",
      "     U.S. NEWS       0.33      0.03      0.05       388\n",
      "      WEDDINGS       0.71      0.72      0.71      1104\n",
      "    WEIRD NEWS       0.34      0.13      0.18       837\n",
      "      WELLNESS       0.41      0.75      0.53      5423\n",
      "         WOMEN       0.35      0.27      0.31      1059\n",
      "    WORLD NEWS       0.48      0.19      0.27       967\n",
      "     WORLDPOST       0.31      0.14      0.19       793\n",
      "\n",
      "      accuracy                           0.53     62859\n",
      "     macro avg       0.48      0.34      0.37     62859\n",
      "  weighted avg       0.52      0.53      0.50     62859\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fb95ae87-4345-4344-8434-07a4a212c531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 53.33%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c459e0-9208-4f2f-bb7b-ac1effb0a583",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "We have gotten a accuracy of 53.33% and this can be improved using:\n",
    "\n",
    "- Hyperparameter Tuning: Use techniques like Grid Search or Random Search to find the best hyperparameters for your Random Forest model. This can include the number of trees, the maximum depth of trees, the minimum samples required to split an internal node, etc.\n",
    "\n",
    "- Feature Engineering: Since text data can be noisy, try to explore different ways to represent your text data, maybe using TF-IDF instead of simple word counts.\n",
    "\n",
    "- Ensemble Methods: Combining multiple models can often yield better results than a single model.\n",
    "\n",
    "#### Some hyperparameters I will tune:\n",
    "\n",
    "- n_estimators: The number of trees in the forest.\n",
    "- max_features: The number of features to consider when looking for the best split.\n",
    "- max_depth: The maximum depth of the trees.\n",
    "- min_samples_split: The minimum number of samples required to split an internal node.\n",
    "- min_samples_leaf: The minimum number of samples required to be at a leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "56e2c016-c857-46e8-8261-f3482bc55134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 5,\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Create the grid search with 3-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, \n",
    "                           cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the grid search model (this will take some time)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4c3baa-9437-4182-ab49-19d8835a2fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5371864013108704"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Random Forest Classifier with the best hyperparameters\n",
    "best_rf_model = RandomForestClassifier(n_estimators=200, max_depth=None, max_features='auto',\n",
    "                                       min_samples_leaf=1, min_samples_split=5, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Train the model on the training set\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_best_rf = best_rf_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "best_rf_accuracy = accuracy_score(y_test, y_pred_best_rf)\n",
    "best_rf_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5204a314-faa7-4b70-8afa-1bdf09a64809",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7a2072-15b6-4ff7-b0d1-86fc84f9e6eb",
   "metadata": {},
   "source": [
    "## Conclusion: News Article Category Classifier\n",
    "\n",
    "### 1. Objective & Dataset:\n",
    "- **Goal**: Build a machine learning classifier to categorize news articles into their respective categories.\n",
    "- **Dataset**: 200,000 news articles with attributes like headline, short description, authors, date, and category.\n",
    "\n",
    "### 2. Data Preprocessing:\n",
    "- Performed extensive preprocessing on the article text:\n",
    "  * Removal of unwanted characters.\n",
    "  * Conversion to lowercase.\n",
    "  * Lemmatization to reduce words to their base form.\n",
    "  * Removal of stopwords.\n",
    "- Utilized the TF-IDF vectorizer for numerical transformation of the cleaned text data.\n",
    "\n",
    "### 3. Model Selection & Training:\n",
    "- Chose the **Random Forest Classifier** due to its ability to handle large datasets and robustness against overfitting.\n",
    "- Initial model yielded an accuracy of ~51%.\n",
    "- Enhanced performance and efficiency via parameter optimization and parallel processing (`n_jobs=-1`).\n",
    "\n",
    "### 4. Hyperparameter Tuning:\n",
    "- Conducted Grid Search with cross-validation for hyperparameter optimization.\n",
    "- Best Parameters:\n",
    "  * `max_depth`: None\n",
    "  * `max_features`: auto\n",
    "  * `min_samples_leaf`: 1\n",
    "  * `min_samples_split`: 5\n",
    "  * `n_estimators`: 200\n",
    "\n",
    "### 5. Final Model Performance:\n",
    "- Refined Random Forest Classifier with the best parameters achieved ~53.7% accuracy on the test dataset.\n",
    "- There's room for further model optimization and exploration of other algorithms.\n",
    "\n",
    "### 6. Future Directions:\n",
    "- Explore other machine learning algorithms like Neural Networks or Gradient Boosted Trees.\n",
    "- Incorporate additional features (e.g., article author, publication date).\n",
    "- Consider advanced preprocessing techniques, such as n-gram based TF-IDF vectorization.\n",
    "\n",
    "---\n",
    "\n",
    "The project highlighted the potential of machine learning in news categorization, with skills and methodologies that are transferable across various domains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ffa77b-f673-4da7-9c10-9af34d10aeb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
